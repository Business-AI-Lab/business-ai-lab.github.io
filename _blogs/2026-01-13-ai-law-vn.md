---
layout: detail
title:
  en: "VIETNAMESE ARTIFICIAL INTELLIGENCE LAW - Some observations on the risk-based management approach"
  vi: "LUẬT TRÍ TUỆ NHÂN TẠO VIỆT NAM - Một số nhận định về cách tiếp cận quản lý rủi ro"  
date: 2026-01-13
author: "Business AI Lab Team"
thumb: "/assets/images/blog/ai-law.jpg"
summary:
  en: "On 10 December 2025, Viet Nam officially passed the Artificial Intelligence Law, which is expected to take effect on 1 March 2026. This is a foundational legal instrument that, for the first time, establishes a unified framework for the management, control, and promotion of the development of artificial intelligence systems in Viet Nam. "
  vi: "Ngày 10/12/2025, Việt Nam đã chính thức thông qua Luật Trí tuệ nhân tạo, dự kiến có hiệu lực từ ngày 01/3/2026. Đây là cơ sở pháp lý nền tảng, lần đầu tiên thiết lập một khuôn khổ thống nhất cho việc quản lý, kiểm soát và khuyến khích phát triển các hệ thống trí tuệ nhân tạo tại Việt Nam."
tags: ["ai-in-law", "blog"]
related_project: "ai-for-business"
readmore:
  en: |
    After three significant rounds of revision, it can be observed that the draft law has been designed to better align with Viet Nam’s practical conditions—a country that currently focuses mainly on the application, deployment, and operation of AI, rather than on the large-scale development of foundational models.
    
    This article focuses on discussing the risk-management approach adopted by the Vietnamese AI Law, clarifying its positive aspects while also pointing out certain limitations that need to be further refined during the implementation process.
    
    1.Some positive aspects of the risk-management approach

    In terms of regulatory form, Viet Nam’s AI Law adopts a risk-based management approach. This approach allows the State to concentrate supervisory resources on AI systems that pose a high risk of significant impact, while limiting unnecessary intervention in low-risk applications. At the initial drafting stage, this regulatory model was constructed in a manner quite similar to the EU AI Act, demonstrating an effort to approximate international standards of AI governance.
    
    In the official version of the law, the risk-classification framework has been adjusted in a more streamlined direction.

    Specifically, the law reduces the number of risk levels from four to three: high risk, medium risk, and low risk (Article 9, Chapter II). At the same time, instead of maintaining a category of “unacceptable risk” as in the November 2025 draft, the law introduces a separate article regulating prohibited acts (Article 7, Chapter I), in order to clearly define the minimum legal boundaries that must be controlled.

    In addition, several types of “unacceptable risks” included in the draft law were removed and do not appear among the prohibited acts in the official law, specifically:

    – Social credit scoring of individuals on a broad scale by state authorities, leading to adverse or unfair treatment in social contexts that are unrelated. (Clause 3, Article 11 of the Draft Law)

    – The use of real-time remote biometric identification systems in public spaces for law-enforcement purposes, except in special cases provided for by sector-specific laws for the prevention and combating of serious crime, and subject to authorization by a competent state authority under a special procedure. (Clause 4, Article 11 of the Draft Law)
    
    – The use of emotion-recognition systems in workplaces and educational institutions, except where permitted by sector-specific laws for medical or safety reasons under strict conditions. (Clause 6, Article 11 of the Draft Law)
    
    It can be observed that the Vietnamese AI Law has been developed in a more streamlined manner compared with the EU AI Act, with a lower level of supervision and intervention.
    
    However, this also raises a question: why were certain risks that are prohibited in the EU included in the draft law but then removed from the official law? And how might the removal of certain AI systems that were previously considered “unacceptable risk” affect the protection of fundamental human rights in the future, especially in the context of the increasingly widespread deployment of AI in the public sector and in state-management activities?
    
    Meanwhile, other prohibited acts under the EU AI Act include:
    - The placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; this prohibition shall not apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity  (point (d), Clause 1, Article 5, EU AI Act)
    - The placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does not cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or categorizing of biometric data in the area of law enforcement (point (g), Clause 1, Article 5, EU AI Act)
    
    2.Some limitations and issues requiring further clarification

    2.1. A framework law heavily dependent on subordinate legislation

    The current AI Law is a framework law, under which many important matters are delegated to the Government and line ministries for detailed regulation. This approach enhances flexibility, but also entails risks if:
    - Subordinate legal instruments are issued slowly;
    - Guidance is inconsistent among authorities;
    - The content of the guidance departs from the law’s spirit of encouraging innovation.
    
    For a rapidly evolving field such as AI, delays or a lack of clarity at the level of subordinate legislation may undermine the regulatory effectiveness of the entire legal framework.
    
    2.2. Unclear boundaries in the post-market supervision regime
    
    Although the Artificial Intelligence Law adopts a management approach based on notification and post-market supervision, several regulatory tools designed in the law, such as:
    - Requirements to establish and retain technical documentation;
    - Obligations to report and provide information upon request;
    - Powers to suspend, withdraw, or re-evaluate systems where necessary.
    
    If these tools are not accompanied by specific guidance on the conditions for application, scope, and degree of intervention, they may lead in practice to overly cautious interpretations and implementation. This may increase compliance costs and affect the pace of testing and deployment of AI systems, particularly for small-scale innovative organisations.
    
    Therefore, the issuance of detailed implementing instruments that transparently define triggering criteria and ensure supervisory principles is crucial to preserving the post-market supervision approach and the law’s objective of promoting innovation.
    
    2.3. The self-risk-classification mechanism requires very clear technical guidance
    
    Pursuant to Clause 1, Article 10 on the classification and notification of artificial intelligence systems, providers are responsible for self-classifying the risk level of AI systems prior to placing them on the market or deploying them. This mechanism helps reduce the administrative burden on enterprises.
    
    However, the effectiveness of this mechanism depends heavily on:
    - The clarity of classification criteria;
    - The issuance of technical annexes and detailed guidelines;
    - Mechanisms for reviewing and correcting misclassification.
    
    According to Clause 5, Article 10, inspection and supervision are conducted based on the system’s risk level:
    a) High-risk systems are subject to periodic inspections or inspections triggered by signs of violations;
    b) Medium-risk systems are supervised through reporting, sample inspections, or assessments by independent organisations.
    
    For point (a), clarification is needed as to:
    - What “periodic” means: annually or according to the usage cycle?
    - What constitutes “signs of violations”: this cannot be left to subjective assessment.
    
    For point (b), clarification is needed as to:
    - What type of reports are required: technical reports, compliance reports, or incident reports?
    - The criteria for sample inspections: random, sector-based, or impact-based?
    - Who qualifies as an “independent organisation”: who accredits it, who pays for the assessment, and who bears responsibility in cases of erroneous evaluation?
    
    2.4. Overlapping responsibilities among stakeholders
    
    The law distinguishes between different actors such as developers, providers, deployers, and users. However, in Article 12 and other provisions, the boundaries of responsibility are not sufficiently clear or robust to allocate liability when incidents occur.
    
    This may create practical difficulties in:
    - Determining whether faults arise from design, data, or use;
    - Identifying primary responsibility in complex incidents;
    - Resolving disputes and compensating for damages.
    
    2.5. The “human-centred” principle has not been sufficiently operationalised
    
    The law affirms the human-centred principle in its programmatic provisions. However, at the level of specific norms, the rights of individuals affected by AI are still addressed mainly indirectly through the obligations of providers and deployers.
    
    The law has not yet clearly established a distinct set of rights for:
    - Individuals evaluated or classified by AI;
    - Individuals affected by AI-influenced decisions;
    - Individuals suffering indirect harm from the operation of AI systems.
    
    There is a need to systematise these into a separate group of rights or a dedicated protection mechanism within the law, and to further concretise this principle through detailed provisions or implementing guidance during the implementation phase.
    
    2.6. Technical terminology lacks clear legal definitions
    
    The AI Law refers to many important concepts such as risk, bias, human oversight, transparency, and accountability. However, these concepts have not yet been clearly defined or accompanied by specific guidance. As a result, in practice, different organisations may interpret and apply them differently, creating difficulties for the consistent deployment and supervision of AI systems.
    
    3.Issues requiring further research

    From the perspective of contributing to the improvement of the law
    - Developing specific criteria for differentiating levels of risk and applying appropriate management measures. This is the direction currently being researched and developed by the Omicron team.
    - Recommending principles for drafting laws governing the application of AI in key sectors such as education, healthcare, law enforcement, and finance.
    - Developing a list of sectors and cases of artificial intelligence applications classified as high risk, including: healthcare; education and competency assessment; transport and the management of critical infrastructure; finance, banking, and credit; labour, recruitment, and human resource management; public administration and essential public services; the judiciary and law enforcement; social security; energy; and other sectors with comparable risks (Clause 2, Article 13 of the Draft Law).
    - Developing a recommended framework for assessing safety, reliability, and transparency in the process of building AI systems.
    - Establishing mechanisms for allocating responsibility among stakeholders in the handling of violations and the resolution of disputes.
    
    From the perspective of contributing to academic research
    - Studying the impact of developing artificial intelligence infrastructure and national autonomous capabilities on socio-economic development.
    - Studying the impact of the use of emotion-recognition systems on user responses, particularly in the healthcare and education sectors.
    - Studying the impact of AI systems on psychological and behavioural outcomes.
    - Assessing the effectiveness of supporting small and medium-sized enterprises in the use of shared digital infrastructure.
    - Research on AI in public governance.
    
    BAI Lab seeks to receive comments, critical feedback, and contributions from the community in order to jointly contribute to the effective and sustainable implementation of the AI Law.
   
    Vietnamese artificial intelligence law: https://duthaoonline.quochoi.vn/dt/luat-tri-tue-nhan-tao/251009091536864496
  vi: |
    Qua ba lần điều chỉnh đáng kể, có thể thấy dự luật đã được thiết kế ngày càng phù hợp hơn với điều kiện thực tiễn của Việt Nam – một quốc gia hiện chủ yếu ứng dụng, triển khai và vận hành AI, thay vì tập trung vào phát triển mô hình nền tảng ở quy mô lớn.

    Bài viết này tập trung thảo luận cách tiếp cận quản lý rủi ro của Luật AI Việt Nam, làm rõ những điểm tích cực, đồng thời chỉ ra một số hạn chế cần tiếp tục hoàn thiện trong quá trình triển khai.
    
    1.Một số điểm tích cực trong cách tiếp cận quản lý rủi ro

    Về hình thức quản lý, luật AI của Việt Nam lựa chọn cách tiếp cận quản lý dựa trên rủi ro. Cách tiếp cận này cho phép Nhà nước tập trung nguồn lực giám sát vào những hệ thống AI có nguy cơ gây tác động lớn, đồng thời hạn chế can thiệp không cần thiết đối với các ứng dụng rủi ro thấp. Ở giai đoạn dự thảo ban đầu, mô hình quản lý này được xây dựng khá tương đồng với EU AI Act, thể hiện nỗ lực tiệm cận các chuẩn mực quản trị AI quốc tế.
    
    Trong phiên bản luật chính thức, khung phân loại rủi ro đã được điều chỉnh theo hướng tinh giản hơn. 

    Cụ thể, luật giảm từ bốn mức rủi ro xuống còn ba mức: rủi ro cao, rủi ro trung bình và rủi ro thấp (Điều 9, Chương II). Đồng thời, thay vì duy trì một nhóm “rủi ro không chấp nhận được” như trong dự thảo tháng 11/2025, luật bổ sung một điều riêng quy định các hành vi bị nghiêm cấm (Điều 7, Chương I), nhằm xác định rõ ranh giới pháp lý tối thiểu cần kiểm soát.

    Bên cạnh đó, một số loại rủi ro không chấp nhận được trong dự luật đã bị lược bỏ và không xuất hiện trong các điều luật các hành vi bị nghiêm cấm tại luật chính thức, cụ thể: 
    - Chấm điểm tín nhiệm xã hội đối với cá nhân trên phạm vi rộng bởi cơ quan nhà nước, dẫn đến sự đối xử bất lợi hoặc không công bằng trong các bối cảnh xã hội không liên quan. (Khoản 3, Điều 11 Dự luật)
    - Sử dụng hệ thống nhận dạng sinh trắc học từ xa theo thời gian thực tại các địa điểm công cộng cho mục đích thực thi pháp luật, trừ các trường hợp đặc biệt do luật chuyên ngành quy định nhằm phòng, chống tội phạm nghiêm trọng và phải được cơ quan nhà nước có thẩm quyền cho phép theo một trình tự đặc biệt. (Khoản 4, Điều 11 Dự luật)
    - Sử dụng hệ thống nhận diện cảm xúc tại nơi làm việc và các cơ sở giáo dục, trừ trường hợp luật chuyên ngành cho phép vì lý do y tế hoặc an toàn với các điều kiện nghiêm ngặt. (Khoản 6 điều 11 Dự luật)
    
    Có thể nhận thấy Luật AI Việt Nam được xây dựng theo hướng tinh giản hơn so với EU AI Act, với mức độ giám sát và can thiệp thấp hơn.

    Tuy nhiên, điều này cũng đặt ra vấn đề: Tại sao có các  rủi ro bị cấm ở EU được đưa vào xây dựng tại dự luật nhưng lại lược bỏ khỏi luật chính thức? Và liệu việc loại bỏ một số hệ thống AI từng được coi là rủi ro không chấp nhận được có thể ảnh hưởng như thế nào đến việc bảo vệ các quyền cơ bản của con người trong tương lai, đặc biệt trong bối cảnh AI ngày càng được triển khai rộng rãi trong khu vực công và các hoạt động quản lý nhà nước.

    Trong khi đó, các hành vi bị nghiêm cấm khác với EU AI Act bao gồm:
    - Việc đưa ra thị trường, đưa vào phục vụ cho mục đích cụ thể này hoặc sử dụng hệ thống AI để đánh giá rủi ro của thể nhân nhằm đánh giá hoặc dự đoán nguy cơ một thể nhân phạm tội hình sự, chỉ dựa trên hồ sơ của một thể nhân hoặc đánh giá các đặc điểm và đặc điểm tính cách của họ; lệnh cấm này sẽ không áp dụng cho các hệ thống AI được sử dụng để hỗ trợ đánh giá của con người về sự tham gia của một người trong một hoạt động tội phạm, vốn đã dựa trên các sự kiện khách quan và có thể kiểm chứng được liên quan trực tiếp đến hoạt động tội phạm.  (điểm (d), Khoản 1, Điều 5, EU AI Act)
    - Đưa ra thị trường, đưa vào phục vụ cho mục đích cụ thể này hoặc sử dụng các hệ thống phân loại sinh trắc học phân loại cá nhân thể nhân dựa trên dữ liệu sinh trắc học của họ để suy luận hoặc suy ra chủng tộc, quan điểm chính trị, thành viên công đoàn, niềm tin tôn giáo hoặc triết học, đời sống tình dục hoặc khuynh hướng tình dục của họ; Lệnh cấm này không bao gồm bất kỳ việc dán nhãn hoặc lọc các bộ dữ liệu sinh trắc học có được hợp pháp, chẳng hạn như hình ảnh, dựa trên dữ liệu sinh trắc học hoặc phân loại dữ liệu sinh trắc học trong lĩnh vực thực thi pháp luật. (điểm (g), Khoản 1, Điều 5, EU AI Act)
    
    2.Một số hạn chế và vấn đề cần tiếp tục làm rõ

    2.1. Luật khung, phụ thuộc nhiều vào văn bản dưới luật

    Luật AI hiện hành mang tính luật khung, trong đó nhiều nội dung quan trọng được giao cho Chính phủ và các bộ, ngành quy định chi tiết. Cách tiếp cận này giúp luật linh hoạt, nhưng cũng tiềm ẩn rủi ro nếu:
    - Văn bản dưới luật ban hành chậm;
    - Hướng dẫn thiếu thống nhất giữa các cơ quan;
    - Nội dung hướng dẫn làm lệch tinh thần khuyến khích đổi mới của luật.
    
    Đối với một lĩnh vực biến động nhanh như AI, sự chậm trễ hoặc thiếu rõ ràng ở cấp dưới luật có thể làm giảm hiệu quả điều chỉnh của toàn bộ khung pháp lý.

    2.2. Ranh giới trong việc hậu kiểm chưa được làm rõ

    Mặc dù Luật Trí tuệ nhân tạo lựa chọn cách tiếp cận quản lý dựa trên thông báo và hậu kiểm, một số công cụ quản lý được thiết kế trong luật như:
    - Yêu cầu lập và lưu giữ hồ sơ kỹ thuật;
    - Nghĩa vụ báo cáo và cung cấp thông tin theo yêu cầu;
    - Thẩm quyền tạm dừng, thu hồi hoặc đánh giá lại hệ thống trong các trường hợp cần thiết.
    
    Nếu không được hướng dẫn cụ thể về điều kiện áp dụng, phạm vi và mức độ can thiệp, có thể dẫn đến cách hiểu và vận hành mang tính thận trọng quá mức trong thực tiễn. Điều này có khả năng làm gia tăng chi phí tuân thủ và ảnh hưởng đến tốc độ thử nghiệm, triển khai các hệ thống AI, đặc biệt đối với các tổ chức đổi mới sáng tạo quy mô nhỏ.
    
    Do đó, việc ban hành các văn bản hướng dẫn chi tiết, minh bạch tiêu chí kích hoạt và bảo đảm nguyên tắc trong giám sát là yếu tố quan trọng để bảo đảm tinh thần hậu kiểm và mục tiêu khuyến khích đổi mới của luật.
    
    2.3. Cơ chế tự phân loại rủi ro đòi hỏi hướng dẫn kỹ thuật rất rõ
    
    Theo Khoản 1 Điều 10:  Phân loại và thông báo hệ thống trí tuệ nhân tạo, nhà cung cấp có trách nhiệm tự phân loại mức độ rủi ro của hệ thống AI trước khi lưu hành hoặc triển khai. Cơ chế này giúp giảm gánh nặng hành chính cho doanh nghiệp.

    Tuy nhiên, hiệu quả của cơ chế này phụ thuộc rất lớn vào:
    - Mức độ rõ ràng của tiêu chí phân loại;
    - Việc ban hành phụ lục kỹ thuật, guideline chi tiết;
    - Cơ chế kiểm tra và hiệu chỉnh phân loại khi có sai lệch.
    
    Theo khoản 5, Điều 10:  Việc kiểm tra, giám sát được thực hiện theo mức rủi ro của hệ thống:
    a) Hệ thống rủi ro cao được kiểm tra định kỳ hoặc khi có dấu hiệu vi phạm;
    b) Hệ thống rủi ro trung bình được giám sát thông qua báo cáo, kiểm tra chọn mẫu hoặc đánh giá của tổ chức độc lập;
    
    Điều a) cần làm rõ: 
    - Định kỳ là bao lâu: theo năm hay theo chu kỳ sử dụng?
    - Dấu hiệu vi phạm là gì: không thể tự đánh giá chủ quan
    
    Điều b) cần làm rõ:
    - Báo cáo là báo cáo gì: báo cáo kỹ thuật, báo cáo tuân thủ hay báo cáo sự cố?
    - Kiểm tra chọn mẫu dựa trên tiêu chí nào: ngẫu nhiên, theo lĩnh vực, theo mức độ tác động?
    - Tổ chức độc lập là ai: ai công nhận, ai trả tiền, ai chịu trách nhiệm nếu đánh giá sai?
    
    2.4. Trách nhiệm giữa các chủ thể còn có dấu hiệu chồng lấn
    
    Luật đã phân biệt các chủ thể như nhà phát triển, nhà cung cấp, bên triển khai và người sử dụng. Tuy nhiên, trong Điều 12 và các điều khoản khác, ranh giới trách nhiệm chưa thật sự rõ ràng và đủ cơ sở để phân định trách nhiệm khi sự cố xảy ra.
    
    Điều này có thể gây khó khăn trong thực tiễn khi:
    - Xác định lỗi phát sinh từ thiết kế, dữ liệu hay cách sử dụng;
    - Phân định trách nhiệm chính trong các sự cố phức tạp;
    - Giải quyết tranh chấp và bồi thường thiệt hại.
    
    2.5. Nguyên tắc “lấy con người làm trung tâm” chưa được cụ thể hóa đầy đủ
    
    Luật đã khẳng định nguyên tắc lấy con người làm trung tâm trong các điều khoản mang tính định hướng. Tuy nhiên, ở cấp độ quy phạm cụ thể, các quyền của cá nhân chịu tác động từ AI vẫn chủ yếu được đề cập gián tiếp thông qua nghĩa vụ của nhà cung cấp và bên triển khai.
    
    Luật hiện chưa thiết lập rõ ràng một hệ thống quyền riêng cho:
    - Người bị đánh giá hoặc xếp loại bởi AI;
    - Người bị ảnh hưởng bởi quyết định có yếu tố AI;
    - Người chịu thiệt hại gián tiếp từ hoạt động của hệ thống AI.
    
    Cần hệ thống hóa thành một nhóm quyền hoặc cơ chế bảo vệ riêng biệt trong luật. Tiếp tục cụ thể hóa nguyên tắc này thông qua các quy định chi tiết hoặc văn bản hướng dẫn trong giai đoạn triển khai.
    
    2.6. Thuật ngữ kỹ thuật còn thiếu định nghĩa pháp lý rõ ràng
    
    Luật AI đã đề cập đến nhiều khái niệm quan trọng như rủi ro, sai lệch, giám sát của con người, minh bạch hay trách nhiệm giải trình. Tuy nhiên, các khái niệm này hiện vẫn chưa được giải thích và hướng dẫn cụ thể, nên khi áp dụng vào thực tế, mỗi tổ chức có thể hiểu và thực hiện khác nhau, gây khó khăn cho việc triển khai và giám sát AI một cách thống nhất.
    
    3.Các vấn đề cần nghiên cứu

    Theo hướng đóng góp cho việc hoàn thiện luật
    - Những tiêu chí cụ thể để phân chia mức độ rủi ro và áp dụng biện pháp quản lý phù hợp. Đây là hướng Omicron team đang nghiên cứu và xây dựng.
    - Khuyến nghị các nguyên tắc xây dựng luật áp dụng AI trong các lĩnh vực quan trọng như: Giáo dục, Y tế, Thực thi pháp luật, Tài chính.
    - Xây dựng danh mục lĩnh vực và trường hợp ứng dụng trí tuệ nhân tạo thuộc diện rủi ro cao: y tế; giáo dục và đánh giá năng lực; giao thông và quản lý hạ tầng quan trọng; tài chính, ngân hàng và tín dụng; lao động, tuyển dụng và quản trị nhân sự; hành chính công và các dịch vụ công thiết yếu; tư pháp và thực thi pháp luật; an sinh xã hội; năng lượng và các lĩnh vực khác có rủi ro tương tự  (Khoản 2, điều 13, dự luật)
    - Xây dựng khung đánh giá tính an toàn, tin cậy và minh bạch khuyến nghị cho quá trình xây dựng hệ thống AI.
    - Xây dựng cơ chế phân định trách nhiệm của đối tượng trong xử lý vi phạm và giải quyết tranh chấp.

    Theo hướng đóng góp cho nghiên cứu khoa học
    - Nghiên cứu tác động của việc xây dựng hạ tầng trí tuệ nhân tạo/năng lực tự chủ đến phát triển kinh tế xã hội 
    - Nghiên cứu tác động của việc sử dụng hệ thống nhận diện cảm xúc đến phản ứng của người dùng, đặc biệt trong lĩnh vực Y tế và Giáo dục
    - Nghiên cứu tác động của hệ thống AI ảnh hưởng đến tâm lý hành vi
    - Đánh giá hiệu quả của việc hỗ trợ doanh nghiệp vừa và nhỏ trong việc sử dụng hạ tầng số dùng chung
    - Nghiên cứu AI trong quản trị công

    BAI Lab mong muốn nhận được ý kiến trao đổi, phản biện và góp ý từ cộng đồng để cùng đóng góp cho việc triển khai Luật AI một cách hiệu quả và bền vững.
    
    Luật trí tuệ nhân tạo Việt Nam 
    https://duthaoonline.quochoi.vn/dt/luat-tri-tue-nhan-tao/251009091536864496

---